---
layout: post
title: 机器学习 准备
category: MachineLearning
tags: MachineLearning
---

##### 学习资料

* Andrew Ng的 [coursera机器学习公开课](https://www.coursera.org/learn/machine-learning)
* [MIT的线性代数公开课](http://web.mit.edu/18.06/www/videos.shtml)
* Peter Harrington的《机器学习实战》和李航的《统计学习方法》
* [知乎精华问答](http://www.zhihu.com/topic/19559450/top-answers)
* 知名博客的博文
* 


#### 数学基础

###### 线性代数

###### 概率与统计

#### 分类

#### 回归

#### 无监督学习

#### 机器学习常用回归方法比较

<table>
   <tr>
      <td></td>
      <td>优点</td>
      <td>缺点</td>
      <td>适用数据</td>
   </tr>
   <tr>
      <td>k-近邻算法</td>
      <td>精度高、对异常值不敏感、无数据输入假定</td>
      <td>计算复杂度高、空间复杂度高</td>
      <td>数值型和标称型</td>
   </tr>
   <tr>
      <td>决策树</td>
      <td>计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据</td>
      <td>可能会产生过度匹配问题</td>
      <td>数值型和标称型</td>
   </tr>
   <tr>
      <td>朴素贝叶斯</td>
      <td>在数据较少的情况下仍然有效，可以处理多类别问题</td>
      <td>对于输入数据的准备方式较为敏感</td>
      <td>标称型数据</td>
   </tr>
   <tr>
      <td>logistic回归</td>
      <td>计算代价不高，易于理解和实现</td>
      <td>容易欠拟合，分类精度可能不高</td>
      <td>数值型和标称型数据</td>
   </tr>
   <tr>
      <td>支持向量机</td>
      <td>泛化错误率低，计算开销不大，结果易于理解</td>
      <td>对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题</td>
      <td>数值型和标称型数据</td>
   </tr>
   <tr>
      <td> AdaBoost元算法 </td>
      <td>泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整</td>
      <td>对离群点敏感</td>
      <td>数值型和标称型数据</td>
   </tr>
</table>